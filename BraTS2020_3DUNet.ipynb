{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BraTS-2020 — 3D U-Net Segmentation (MONAI + TorchIO)\n",
    "\n",
    "**Author:** Ismail Berriss / Soufiane Jadda  \n",
    "**Date:** 2025-11-26  \n",
    "**Purpose:** End-to-end 3D brain tumor segmentation pipeline using the BraTS-2020 dataset and a 3D U-Net model.\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "This notebook implements a complete workflow for multi-modal brain tumor segmentation:\n",
    "\n",
    "1. **Setup & Verification** — Install dependencies, detect GPU, set seeds\n",
    "2. **Data Loading & Preprocessing** — Load BraTS NIfTI volumes, normalize, resample, crop\n",
    "3. **Model Architecture** — 3D U-Net with configurable channels and depth\n",
    "4. **Training Pipeline** — Patch-based training with augmentations, loss tracking, validation\n",
    "5. **Evaluation & Metrics** — Dice score, Hausdorff95, per-region metrics (WT, TC, ET)\n",
    "6. **Inference & Post-processing** — Sliding-window prediction, connected-component filtering\n",
    "7. **Uncertainty Estimation** — Monte Carlo Dropout for voxel-wise uncertainty\n",
    "8. **Visualization & Export** — Result overlays, uncertainty heatmaps, artifact export\n",
    "\n",
    "**Estimated runtime:** ~30-60 minutes for a single epoch on a 24GB GPU (full BraTS-2020 training set). Use the \"Quick Debug\" mode to test the pipeline in ~5-10 minutes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Installation & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# This cell installs all required packages for the notebook.\n",
    "# If running in Colab, uncomment the pip commands. For local installs, use conda.\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages_to_install = [\n",
    "    'torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118',\n",
    "    'monai[all]',\n",
    "    'torchio',\n",
    "    'nibabel',\n",
    "    'numpy pandas scipy scikit-learn matplotlib seaborn',\n",
    "    'tqdm',\n",
    "    'pytest',  # for validation\n",
    "]\n",
    "\n",
    "print(\"Installing dependencies... This may take a few minutes.\")\n",
    "for package in packages_to_install:\n",
    "    print(f\"\\n>>> Installing: {package}\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q'] + package.split())\n",
    "\n",
    "print(\"\\n✓ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Environment Information | 2025-11-29 21:11:48\n",
      "======================================================================\n",
      "Python version:           3.13.5\n",
      "PyTorch version:          2.9.0+cu130\n",
      "NumPy version:            2.1.3\n",
      "Nibabel version:          5.3.2\n",
      "TorchIO version:          0.21.0\n",
      "MONAI version:            1.5.1\n",
      "Pandas version:           2.2.3\n",
      "======================================================================\n",
      "\n",
      "✓ CUDA is available!\n",
      "  GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "  VRAM: 4.3 GB\n",
      "\n",
      "Device selected: cuda\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Import core libraries and print versions\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchio as tio\n",
    "import nibabel as nib\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.transforms import Compose, EnsureChannelFirst, NormalizeIntensity, Spacing\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import KFold\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from datetime import datetime\n",
    "\n",
    "# Version printing\n",
    "print(\"=\"*70)\n",
    "print(f\"Environment Information | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Python version:           {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch version:          {torch.__version__}\")\n",
    "print(f\"NumPy version:            {np.__version__}\")\n",
    "print(f\"Nibabel version:          {nib.__version__}\")\n",
    "print(f\"TorchIO version:          {tio.__version__}\")\n",
    "print(f\"MONAI version:            {__import__('monai').__version__}\")\n",
    "print(f\"Pandas version:           {pd.__version__}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# GPU Detection\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\\n✓ CUDA is available!\")\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"\\n⚠ CUDA not available. Using CPU. Training will be very slow.\")\n",
    "\n",
    "print(f\"\\nDevice selected: {device}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Random seeds set to 42 for reproducibility.\n",
      "  Note: deterministic mode may reduce training speed.\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# PyTorch\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Deterministic behavior (may reduce performance)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"✓ Random seeds set to {SEED} for reproducibility.\")\n",
    "print(\"  Note: deterministic mode may reduce training speed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Configuration & Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CONFIGURATION SUMMARY\n",
      "======================================================================\n",
      "DATA_ROOT............................... ./dataset/MICCAI_BraTS2020_TrainingData\n",
      "OUTPUT_DIR.............................. ./output\n",
      "CHECKPOINT_DIR.......................... ./checkpoint\n",
      "DEBUG_MODE.............................. True\n",
      "PATCH_SIZE.............................. (128, 128, 128)\n",
      "BATCH_SIZE.............................. 1\n",
      "NUM_EPOCHS.............................. 50\n",
      "LEARNING_RATE........................... 0.001\n",
      "WEIGHT_DECAY............................ 1e-05\n",
      "NUM_WORKERS............................. 0\n",
      "IN_CHANNELS............................. 4\n",
      "OUT_CHANNELS............................ 4\n",
      "INITIAL_FEATURES........................ 32\n",
      "TARGET_SPACING.......................... (1.0, 1.0, 1.0)\n",
      "MODALITIES.............................. ['t1', 't1ce', 't2', 'flair']\n",
      "CLASS_NAMES............................. ['Background', 'NCR/NET', 'ED', 'ET']\n",
      "INFERENCE_PATCH_SIZE.................... (128, 128, 128)\n",
      "INFERENCE_OVERLAP....................... 0.5\n",
      "MC_DROPOUT_PASSES....................... 20\n",
      "VALIDATION_SPLIT........................ 0.2\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Global configuration and hyperparameters\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Central configuration for the entire pipeline.\"\"\"\n",
    "    \n",
    "    # === Dataset Paths ===\n",
    "    # Set DATA_ROOT to your local BraTS-2020 directory structure:\n",
    "    #   dataset/\n",
    "    #   └── MICCAI_BraTS2020_TrainingData/\n",
    "    #       ├── BraTS20_Training_001/\n",
    "    #       │   ├── BraTS20_Training_001_flair.nii\n",
    "    #       │   ├── BraTS20_Training_001_seg.nii\n",
    "    #       │   ├── BraTS20_Training_001_t1.nii\n",
    "    #       │   ├── BraTS20_Training_001_t1ce.nii\n",
    "    #       │   ├── BraTS20_Training_001_t2.nii\n",
    "    #       ├── BraTS20_Training_002/\n",
    "    #       └── ...\n",
    "    DATA_ROOT = './dataset/MICCAI_BraTS2020_TrainingData'\n",
    "    OUTPUT_DIR = './output'\n",
    "    CHECKPOINT_DIR = './checkpoint'\n",
    "    \n",
    "    # === Quick Debug Mode ===\n",
    "    # Set to True to use only 2 subjects and 1 epoch for testing\n",
    "    DEBUG_MODE = True  # Change to True for quick testing\n",
    "    \n",
    "    # === Training Hyperparameters ===\n",
    "    PATCH_SIZE = (128, 128, 128)  # Patch size for patch-based training\n",
    "    BATCH_SIZE = 1  # Batch size (reduce if OOM: 1 or 2)\n",
    "    NUM_EPOCHS = 50  # Number of training epochs\n",
    "    LEARNING_RATE = 1e-3  # Initial learning rate\n",
    "    WEIGHT_DECAY = 1e-5  # L2 regularization\n",
    "    NUM_WORKERS = 0  # DataLoader workers\n",
    "    \n",
    "    # === Model Configuration ===\n",
    "    IN_CHANNELS = 4  # T1, T1ce, T2, FLAIR\n",
    "    OUT_CHANNELS = 4  # Background, NCR/NET, ED, ET\n",
    "    INITIAL_FEATURES = 32  # Starting feature maps for U-Net\n",
    "    \n",
    "    # === Data Configuration ===\n",
    "    TARGET_SPACING = (1.0, 1.0, 1.0)  # Isotropic 1mm\n",
    "    MODALITIES = ['t1', 't1ce', 't2', 'flair']  # Modality order\n",
    "    \n",
    "    # === Class Labels (BraTS) ===\n",
    "    # 0 = Background, 1 = NCR/NET, 2 = ED, 4 = ET (merged to 3)\n",
    "    CLASS_NAMES = ['Background', 'NCR/NET', 'ED', 'ET']\n",
    "    \n",
    "    # === Inference ===\n",
    "    INFERENCE_PATCH_SIZE = (128, 128, 128)\n",
    "    INFERENCE_OVERLAP = 0.5  # Overlap ratio for sliding window\n",
    "    MC_DROPOUT_PASSES = 20  # Passes for Monte Carlo Dropout uncertainty\n",
    "    \n",
    "    # === Validation ===\n",
    "    VALIDATION_SPLIT = 0.2  # 80% train, 20% val\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_config():\n",
    "        \"\"\"Print all configuration parameters.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"CONFIGURATION SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        for key, value in Config.__dict__.items():\n",
    "            if not key.startswith('_') and not callable(value):\n",
    "                print(f\"{key:.<40} {value}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Print config\n",
    "Config.print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Output directory: ./output\n",
      "✓ Checkpoint directory: ./checkpoint\n"
     ]
    }
   ],
   "source": [
    "# Create necessary directories\n",
    "\n",
    "os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Output directory: {Config.OUTPUT_DIR}\")\n",
    "print(f\"✓ Checkpoint directory: {Config.CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATASET VERIFICATION\n",
      "======================================================================\n",
      "Data root: dataset\\MICCAI_BraTS2020_TrainingData\n",
      "Total subjects found: 369\n",
      "\n",
      "Sample subjects (first 5):\n",
      "  - BraTS20_Training_001\n",
      "  - BraTS20_Training_002\n",
      "  - BraTS20_Training_003\n",
      "  - BraTS20_Training_004\n",
      "  - BraTS20_Training_005\n",
      "======================================================================\n",
      "\n",
      "✓ Dataset verified: 369 subjects ready for training\n"
     ]
    }
   ],
   "source": [
    "# Verify BraTS-2020 dataset structure\n",
    "\n",
    "def verify_dataset_structure(data_root):\n",
    "    \"\"\"\n",
    "    Verify that the BraTS-2020 dataset is properly structured.\n",
    "    Raises an error if the dataset is not found or malformed.\n",
    "    \"\"\"\n",
    "    data_root = Path(data_root)\n",
    "    \n",
    "    if not data_root.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Dataset directory not found: {data_root}\\n\\n\"\n",
    "            \"Please update Config.DATA_ROOT to point to your BraTS2020 folder.\\n\\n\"\n",
    "            \"Expected structure:\\n\"\n",
    "            \"  dataset/\\n\"\n",
    "            \"  └──MICCAI_BraTS2020_TrainingData/\\n\"\n",
    "            \"       ├── BraTS20_Training_001/\\n\"\n",
    "            \"       │   ├── BraTS20_Training_001_flair.nii\\n\"\n",
    "            \"       │   ├── BraTS20_Training_001_seg.nii\\n\"\n",
    "            \"       │   ├── BraTS20_Training_001_t1.nii\\n\"\n",
    "            \"       │   ├── BraTS20_Training_001_t1ce.nii\\n\"\n",
    "            \"       │   ├── BraTS20_Training_001_t2.nii\\n\"\n",
    "            \"       ├── BraTS20_Training_002/\\n\"\n",
    "            \"       └── ...\"\n",
    "        )\n",
    "    \n",
    "    subject_folders = [f for f in data_root.iterdir() if f.is_dir() and f.name.startswith('BraTS20_Training')]\n",
    "\n",
    "    \n",
    "    if len(subject_folders) == 0:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No BraTS20_Training_* folders found in {data_root}\\n\"\n",
    "            \"Please check your DATA_ROOT path.\"\n",
    "        )\n",
    "    \n",
    "    # Verify first subject has all required files\n",
    "    test_subject = subject_folders[0]\n",
    "    required_files = ['t1.nii', 't1ce.nii', 't2.nii', 'flair.nii', 'seg.nii']\n",
    "    \n",
    "    for mod in required_files:\n",
    "        expected_file = test_subject / f\"{test_subject.name}_{mod}\"\n",
    "        if not expected_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Missing file: {expected_file}\\n\"\n",
    "                \"Each subject folder should contain: t1, t1ce, t2, flair, and seg .nii files\"\n",
    "            )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DATASET VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Data root: {data_root}\")\n",
    "    print(f\"Total subjects found: {len(subject_folders)}\")\n",
    "    \n",
    "    # Show sample subjects\n",
    "    print(f\"\\nSample subjects (first 5):\")\n",
    "    for subject_folder in sorted(subject_folders)[:5]:\n",
    "        print(f\"  - {subject_folder.name}\")\n",
    "    \n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return subject_folders\n",
    "\n",
    "# Run verification\n",
    "try:\n",
    "    subject_folders = verify_dataset_structure(Config.DATA_ROOT)\n",
    "    print(f\"✓ Dataset verified: {len(subject_folders)} subjects ready for training\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n❌ ERROR: {e}\")\n",
    "    print(\"\\nPlease update Config.DATA_ROOT in the cell above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Data Preprocessing & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BraTS label utilities loaded\n"
     ]
    }
   ],
   "source": [
    "# BraTS Label Mapping and Utilities\n",
    "\n",
    "class BraTSLabels:\n",
    "    \"\"\"\n",
    "    BraTS label definitions and mappings.\n",
    "    Original BraTS labels: 0=background, 1=NCR/NET, 2=ED, 4=ET\n",
    "    We remap to: 0=background, 1=NCR/NET, 2=ED, 3=ET\n",
    "    \"\"\"\n",
    "    BACKGROUND = 0\n",
    "    NCR_NET = 1      # Necrotic and Non-Enhancing Tumor\n",
    "    ED = 2           # Peritumoral Edema\n",
    "    ET = 3           # Enhancing Tumor (remapped from 4 to 3)\n",
    "    \n",
    "    # Region definitions for evaluation metrics\n",
    "    # Whole Tumor (WT): NCR/NET + ED + ET = labels {1, 2, 3}\n",
    "    # Tumor Core (TC): NCR/NET + ET = labels {1, 3}\n",
    "    # Enhancing Tumor (ET): ET only = label {3}\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_region_mask(segmentation, region='WT'):\n",
    "        \"\"\"\n",
    "        Extract a specific region from multi-class segmentation.\n",
    "        \n",
    "        Args:\n",
    "            segmentation: (H, W, D) array with class labels\n",
    "            region: 'WT' (Whole Tumor), 'TC' (Tumor Core), or 'ET' (Enhancing Tumor)\n",
    "        \n",
    "        Returns:\n",
    "            Binary mask for the region\n",
    "        \"\"\"\n",
    "        if region == 'WT':  # Whole Tumor\n",
    "            return np.isin(segmentation, [1, 2, 3]).astype(np.uint8)\n",
    "        elif region == 'TC':  # Tumor Core\n",
    "            return np.isin(segmentation, [1, 3]).astype(np.uint8)\n",
    "        elif region == 'ET':  # Enhancing Tumor\n",
    "            return (segmentation == 3).astype(np.uint8)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown region: {region}\")\n",
    "\n",
    "print(\"✓ BraTS label utilities loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessing utilities loaded\n"
     ]
    }
   ],
   "source": [
    "# Data Loading and Preprocessing Functions\n",
    "\n",
    "def load_subject_volumes(data_root, subject_folder_name):\n",
    "    \"\"\"\n",
    "    Load all modalities (T1, T1ce, T2, FLAIR) and segmentation label for a subject.\n",
    "    \n",
    "    Args:\n",
    "        data_root: Path to MICCAI_BraTS2020_TrainingData folder\n",
    "        subject_folder_name: Subject folder name (e.g., 'BraTS20_Training_001')\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'volumes' (4, H, W, D) and 'label' (H, W, D)\n",
    "    \"\"\"\n",
    "    data_root = Path(data_root)\n",
    "    subject_folder = data_root / subject_folder_name\n",
    "    \n",
    "    if not subject_folder.exists():\n",
    "        raise FileNotFoundError(f\"Subject folder not found: {subject_folder}\")\n",
    "    \n",
    "    modalities = ['t1', 't1ce', 't2', 'flair']\n",
    "    volumes = []\n",
    "    \n",
    "    for mod in modalities:\n",
    "        file_path = subject_folder / f\"{subject_folder_name}_{mod}.nii\"\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing: {file_path}\")\n",
    "        \n",
    "        img = nib.load(file_path)\n",
    "        data = img.get_fdata().astype(np.float32)\n",
    "        volumes.append(data)\n",
    "    \n",
    "    # Stack modalities: (4, H, W, D)\n",
    "    volumes_array = np.stack(volumes, axis=0)\n",
    "    \n",
    "    # Load segmentation label\n",
    "    label_path = subject_folder / f\"{subject_folder_name}_seg.nii\"\n",
    "    label_img = nib.load(label_path)\n",
    "    label_data = label_img.get_fdata().astype(np.uint8)\n",
    "    \n",
    "    # Remap label 4 (ET) to 3 for consistency\n",
    "    label_data[label_data == 4] = 3\n",
    "    \n",
    "    return {\n",
    "        'volumes': volumes_array,\n",
    "        'label': label_data,\n",
    "        'affine': img.affine,\n",
    "        'header': img.header\n",
    "    }\n",
    "\n",
    "\n",
    "def normalize_modality(volume, mask=None, use_nonzero_mask=True):\n",
    "    \"\"\"\n",
    "    Z-score normalize a single modality.\n",
    "    Can optionally use a brain mask to exclude background voxels.\n",
    "    \n",
    "    Args:\n",
    "        volume: (H, W, D) array\n",
    "        mask: Optional (H, W, D) binary mask for normalization region\n",
    "        use_nonzero_mask: If True and mask is None, use nonzero voxels\n",
    "    \n",
    "    Returns:\n",
    "        Normalized volume\n",
    "    \"\"\"\n",
    "    if use_nonzero_mask and mask is None:\n",
    "        mask = volume > 0\n",
    "    \n",
    "    if mask is not None:\n",
    "        masked_volume = volume[mask]\n",
    "        if len(masked_volume) > 0:\n",
    "            mean = masked_volume.mean()\n",
    "            std = masked_volume.std()\n",
    "            if std > 0:\n",
    "                volume = (volume - mean) / std\n",
    "    else:\n",
    "        mean = volume.mean()\n",
    "        std = volume.std()\n",
    "        if std > 0:\n",
    "            volume = (volume - mean) / std\n",
    "    \n",
    "    return volume\n",
    "\n",
    "\n",
    "def preprocess_subject(subject_data, target_spacing=(1.0, 1.0, 1.0)):\n",
    "    \"\"\"\n",
    "    Preprocess a subject: normalize modalities, resample to isotropic spacing.\n",
    "    \n",
    "    Args:\n",
    "        subject_data: Dict with 'volumes', 'label', 'affine'\n",
    "        target_spacing: Target voxel spacing in mm\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed subject data\n",
    "    \"\"\"\n",
    "    volumes = subject_data['volumes'].copy()  # (4, H, W, D)\n",
    "    label = subject_data['label'].copy()       # (H, W, D)\n",
    "    \n",
    "    # Normalize each modality independently\n",
    "    brain_mask = volumes[0] > 0  # Use T1 as reference for brain mask\n",
    "    for i in range(volumes.shape[0]):\n",
    "        volumes[i] = normalize_modality(volumes[i], mask=brain_mask)\n",
    "    \n",
    "    # Optional: Resample to isotropic 1mm (can be expensive, skip for now)\n",
    "    # For speed, we'll just normalize and crop\n",
    "    \n",
    "    return {\n",
    "        'volumes': volumes,\n",
    "        'label': label,\n",
    "        'affine': subject_data['affine']\n",
    "    }\n",
    "\n",
    "print(\"✓ Preprocessing utilities loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BraTSDataset class loaded\n"
     ]
    }
   ],
   "source": [
    "# Custom Dataset for patch-based training\n",
    "\n",
    "class BraTSDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for BraTS 3D volumes with patch-based sampling.\n",
    "    Loads subjects on demand and extracts random patches during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root,\n",
    "        subject_folder_names,\n",
    "        patch_size=(128, 128, 128),\n",
    "        augment=False,\n",
    "        num_patches_per_subject=1,\n",
    "        cache_size=0,\n",
    "    ):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.subject_ids = list(subject_folder_names)\n",
    "        self.subject_folder_names = subject_folder_names\n",
    "        self.patch_size = patch_size\n",
    "        self.augment = augment\n",
    "        self.num_patches_per_subject = num_patches_per_subject\n",
    "        self.cache_size = cache_size\n",
    "        \n",
    "        # # Preload and cache all subject data\n",
    "        # self.subject_data_cache = {}\n",
    "        # self._load_all_subjects()\n",
    "        \n",
    "        # If caching is enabled, wrap load_subject in LRU cache\n",
    "        if cache_size > 0:\n",
    "            from functools import lru_cache\n",
    "            self._load_subject_cached = lru_cache(maxsize=cache_size)(self._load_subject_uncached)\n",
    "        else:\n",
    "            self._load_subject_cached = self._load_subject_uncached\n",
    "    \n",
    "    def _load_subject_uncached(self, subject_id):\n",
    "        \"\"\"Load and preprocess a single subject — NOT cached by default.\"\"\"\n",
    "        raw_data = load_subject_volumes(self.data_root, subject_id)\n",
    "        return preprocess_subject(raw_data)\n",
    "    \n",
    "    # def _load_all_subjects(self):\n",
    "    #     \"\"\"Load and preprocess all subjects into memory.\"\"\"\n",
    "    #     print(f\"Loading {len(self.subject_ids)} subjects...\")\n",
    "    #     for subject_id in tqdm(self.subject_ids, desc=\"Loading subjects\"):\n",
    "    #         try:\n",
    "    #             raw_data = load_subject_volumes(\n",
    "    #                 self.image_dir, self.label_dir, subject_id\n",
    "    #             )\n",
    "    #             processed = preprocess_subject(raw_data)\n",
    "    #             self.subject_data_cache[subject_id] = processed\n",
    "    #         except Exception as e:\n",
    "    #             print(f\"Warning: Failed to load {subject_id}: {e}\")\n",
    "    \n",
    "    def _extract_random_patch(self, volumes, label, idx=0):\n",
    "        \"\"\"\n",
    "        Extract a random patch from a volume.\n",
    "        If the volume is smaller than patch_size, pad it.\n",
    "        \"\"\"\n",
    "        _, h, w, d = volumes.shape\n",
    "        ph, pw, pd = self.patch_size\n",
    "        \n",
    "        # Ensure volume is at least patch_size\n",
    "        pad_h = max(0, ph - h)\n",
    "        pad_w = max(0, pw - w)\n",
    "        pad_d = max(0, pd - d)\n",
    "        if pad_h or pad_w or pad_d:\n",
    "            volumes = np.pad(\n",
    "                volumes,\n",
    "                ((0, 0), (0, pad_h), (0, pad_w), (0, pad_d)),\n",
    "                mode='constant'\n",
    "            )\n",
    "            label = np.pad(\n",
    "                label,\n",
    "                ((0, pad_h), (0, pad_w), (0, pad_d)),\n",
    "                mode='constant'\n",
    "            )\n",
    "            _, h, w, d = volumes.shape\n",
    "        \n",
    "        _, h, w, d = volumes.shape\n",
    "        \n",
    "        # Random patch location\n",
    "        np.random.seed(42 + idx)\n",
    "        torch.manual_seed(42 + idx)\n",
    "        \n",
    "        h_start = np.random.randint(0, h - ph + 1) if h > ph else 0\n",
    "        w_start = np.random.randint(0, w - pw + 1) if w > pw else 0\n",
    "        d_start = np.random.randint(0, d - pd + 1) if d > pd else 0\n",
    "        \n",
    "        print(f\"h_start: {h_start}\")\n",
    "        print(f\"h_start: {w_start}\")\n",
    "        print(f\"h_start: {d_start}\")\n",
    "        \n",
    "        patch_volumes = volumes[\n",
    "            :, h_start:h_start+ph, w_start:w_start+pw, d_start:d_start+pd\n",
    "        ]\n",
    "        patch_label = label[\n",
    "            h_start:h_start+ph, w_start:w_start+pw, d_start:d_start+pd\n",
    "        ]\n",
    "        \n",
    "        return patch_volumes, patch_label\n",
    "    \n",
    "    def _apply_augmentations(self, volumes, label):\n",
    "        \"\"\"\n",
    "        Apply data augmentations:\n",
    "        - Random flip (left-right)\n",
    "        - Random rotation (small angles)\n",
    "        - Random gamma adjustment\n",
    "        \"\"\"\n",
    "        # Random flip along H axis (left-right)\n",
    "        if np.random.rand() > 0.5:\n",
    "            volumes = np.flip(volumes, axis=1).copy()\n",
    "            label = np.flip(label, axis=0).copy()\n",
    "        \n",
    "        # Random gamma adjustment (intensity correction)\n",
    "        if np.random.rand() > 0.5:\n",
    "            gamma = np.random.uniform(0.7, 1.3)\n",
    "            volumes = np.power(np.maximum(volumes, 0), gamma)\n",
    "        \n",
    "        # Random small rotation (via TorchIO would be better, but for speed we keep it simple)\n",
    "        if np.random.rand() > 0.7:\n",
    "            # Small rotation around Z axis\n",
    "            from scipy import ndimage\n",
    "            angle = np.random.uniform(-15, 15)\n",
    "            for i in range(volumes.shape[0]):\n",
    "                volumes[i] = ndimage.rotate(\n",
    "                    volumes[i], angle, axes=(0, 1), order=1, reshape=False\n",
    "                )\n",
    "            label = ndimage.rotate(\n",
    "                label, angle, axes=(0, 1), order=0, reshape=False\n",
    "            )\n",
    "        \n",
    "        return volumes, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.subject_ids) * self.num_patches_per_subject\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        subject_idx = idx // self.num_patches_per_subject\n",
    "        patch_idx = idx % self.num_patches_per_subject  # unused, but keeps indexing clear\n",
    "        subject_id = self.subject_ids[subject_idx]\n",
    "        \n",
    "        try:\n",
    "            subject_data = self._load_subject_cached(subject_id)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load subject '{subject_id}': {e}\") from e  \n",
    "        \n",
    "        volumes = subject_data['volumes'].copy()\n",
    "        label = subject_data['label'].copy()\n",
    "        \n",
    "        # Extract random patch\n",
    "        patch_volumes, patch_label = self._extract_random_patch(volumes, label, idx=subject_idx)\n",
    "        \n",
    "        # Apply augmentations if training\n",
    "        if self.augment:\n",
    "            patch_volumes, patch_label = self._apply_augmentations(\n",
    "                patch_volumes, patch_label\n",
    "            )\n",
    "        \n",
    "        # Convert to tensors\n",
    "        volumes_tensor = torch.from_numpy(patch_volumes).float()\n",
    "        label_tensor = torch.from_numpy(patch_label).long()\n",
    "        \n",
    "        return {\n",
    "            'image': volumes_tensor,\n",
    "            'label': label_tensor,\n",
    "            'subject_id': subject_id\n",
    "        }\n",
    "\n",
    "print(\"✓ BraTSDataset class loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Model Architecture (3D U-Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL ARCHITECTURE\n",
      "======================================================================\n",
      "Total Parameters: 4,748,837\n",
      "Trainable Parameters: 4,748,837\n",
      "\n",
      "Model Architecture:\n",
      "UNet(\n",
      "  (model): Sequential(\n",
      "    (0): ResidualUnit(\n",
      "      (conv): Sequential(\n",
      "        (unit0): Convolution(\n",
      "          (conv): Conv3d(4, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (adn): ADN(\n",
      "            (N): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (D): Dropout(p=0.1, inplace=False)\n",
      "            (A): PReLU(num_parameters=1)\n",
      "          )\n",
      "        )\n",
      "        (unit1): Convolution(\n",
      "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (adn): ADN(\n",
      "            (N): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (D): Dropout(p=0.1, inplace=False)\n",
      "            (A): PReLU(num_parameters=1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (residual): Conv3d(4, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "    )\n",
      "    (1): SkipConnection(\n",
      "      (submodule): Sequential(\n",
      "        (0): ResidualUnit(\n",
      "          (conv): Sequential(\n",
      "            (unit0): Convolution(\n",
      "              (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "              (adn): ADN(\n",
      "                (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                (D): Dropout(p=0.1, inplace=False)\n",
      "                (A): PReLU(num_parameters=1)\n",
      "              )\n",
      "            )\n",
      "            (unit1): Convolution(\n",
      "              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "              (adn): ADN(\n",
      "                (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                (D): Dropout(p=0.1, inplace=False)\n",
      "                (A): PReLU(num_parameters=1)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (residual): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "        )\n",
      "        (1): SkipConnection(\n",
      "          (submodule): Sequential(\n",
      "            (0): ResidualUnit(\n",
      "              (conv): Sequential(\n",
      "                (unit0): Convolution(\n",
      "                  (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "                  (adn): ADN(\n",
      "                    (N): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                    (D): Dropout(p=0.1, inplace=False)\n",
      "                    (A): PReLU(num_parameters=1)\n",
      "                  )\n",
      "                )\n",
      "                (unit1): Convolution(\n",
      "                  (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "                  (adn): ADN(\n",
      "                    (N): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                    (D): Dropout(p=0.1, inplace=False)\n",
      "                    (A): PReLU(num_parameters=1)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (residual): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "            )\n",
      "            (1): SkipConnection(\n",
      "              (submodule): ResidualUnit(\n",
      "                (conv): Sequential(\n",
      "                  (unit0): Convolution(\n",
      "                    (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "                    (adn): ADN(\n",
      "                      (N): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                      (D): Dropout(p=0.1, inplace=False)\n",
      "                      (A): PReLU(num_parameters=1)\n",
      "                    )\n",
      "                  )\n",
      "                  (unit1): Convolution(\n",
      "                    (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "                    (adn): ADN(\n",
      "                      (N): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                      (D): Dropout(p=0.1, inplace=False)\n",
      "                      (A): PReLU(num_parameters=1)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (residual): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "              )\n",
      "            )\n",
      "            (2): Sequential(\n",
      "              (0): Convolution(\n",
      "                (conv): ConvTranspose3d(384, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "                (adn): ADN(\n",
      "                  (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                  (D): Dropout(p=0.1, inplace=False)\n",
      "                  (A): PReLU(num_parameters=1)\n",
      "                )\n",
      "              )\n",
      "              (1): ResidualUnit(\n",
      "                (conv): Sequential(\n",
      "                  (unit0): Convolution(\n",
      "                    (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "                    (adn): ADN(\n",
      "                      (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                      (D): Dropout(p=0.1, inplace=False)\n",
      "                      (A): PReLU(num_parameters=1)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (residual): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Convolution(\n",
      "            (conv): ConvTranspose3d(128, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "            (adn): ADN(\n",
      "              (N): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "              (D): Dropout(p=0.1, inplace=False)\n",
      "              (A): PReLU(num_parameters=1)\n",
      "            )\n",
      "          )\n",
      "          (1): ResidualUnit(\n",
      "            (conv): Sequential(\n",
      "              (unit0): Convolution(\n",
      "                (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "                (adn): ADN(\n",
      "                  (N): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                  (D): Dropout(p=0.1, inplace=False)\n",
      "                  (A): PReLU(num_parameters=1)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (residual): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Convolution(\n",
      "        (conv): ConvTranspose3d(64, 4, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "        (adn): ADN(\n",
      "          (N): InstanceNorm3d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          (D): Dropout(p=0.1, inplace=False)\n",
      "          (A): PReLU(num_parameters=1)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualUnit(\n",
      "        (conv): Sequential(\n",
      "          (unit0): Convolution(\n",
      "            (conv): Conv3d(4, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          )\n",
      "        )\n",
      "        (residual): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build 3D U-Net Model\n",
    "\n",
    "def build_unet_model(\n",
    "    in_channels=4,\n",
    "    out_channels=4,\n",
    "    initial_features=32,\n",
    "    dropout_rate=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a 3D U-Net model using MONAI.\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Number of input channels (4 for BraTS)\n",
    "        out_channels: Number of output classes (4 for BraTS)\n",
    "        initial_features: Number of features in first conv layer\n",
    "        dropout_rate: Dropout rate for Monte Carlo uncertainty\n",
    "    \n",
    "    Returns:\n",
    "        UNet model\n",
    "    \"\"\"\n",
    "    model = UNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        channels=(32, 64, 128, 256),\n",
    "        strides=(2, 2, 2),\n",
    "        num_res_units=2,\n",
    "        dropout=dropout_rate,  # Needed for MC Dropout\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "model = build_unet_model(\n",
    "    in_channels=Config.IN_CHANNELS,\n",
    "    out_channels=Config.OUT_CHANNELS,\n",
    "    initial_features=Config.INITIAL_FEATURES,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Loss Function & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loss function: DiceCELoss\n",
      "✓ Optimizer: AdamW\n",
      "  Learning Rate: 0.001\n",
      "  Weight Decay: 1e-05\n",
      "✓ Scheduler: CosineAnnealingLR\n"
     ]
    }
   ],
   "source": [
    "# Loss Function and Optimizer Setup\n",
    "\n",
    "# Combined loss: Dice + CrossEntropy\n",
    "loss_function = DiceCELoss(\n",
    "    include_background=False,  # Ignore background class\n",
    "    to_onehot_y=True,\n",
    "    softmax=True,\n",
    "    weight=None,\n",
    "    reduction='mean'\n",
    ")\n",
    "\n",
    "# Optimizer: AdamW with learning rate scheduling\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=Config.LEARNING_RATE,\n",
    "    weight_decay=Config.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Learning rate scheduler: CosineAnnealingLR\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=Config.NUM_EPOCHS,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "print(\"✓ Loss function: DiceCELoss\")\n",
    "print(\"✓ Optimizer: AdamW\")\n",
    "print(f\"  Learning Rate: {Config.LEARNING_RATE}\")\n",
    "print(f\"  Weight Decay: {Config.WEIGHT_DECAY}\")\n",
    "print(\"✓ Scheduler: CosineAnnealingLR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Metric functions loaded\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "def compute_dice_score(pred_mask, target_mask):\n",
    "    \"\"\"\n",
    "    Compute Dice score for a specific class.\n",
    "    \n",
    "    Args:\n",
    "        pred: (H, W, D) predicted class\n",
    "        target: (H, W, D) ground truth class\n",
    "    \n",
    "    Returns:\n",
    "        Dice score (float)\n",
    "    \"\"\"\n",
    "    intersection = np.sum(pred_mask * target_mask)\n",
    "    denominator = np.sum(pred_mask) + np.sum(target_mask)\n",
    "    \n",
    "    if denominator == 0:\n",
    "        # return 1.0 if np.sum(target_mask) == 0 else 0.0\n",
    "        return 1.0 if np.p.array_equal(pred_mask, target_mask) == 0 else 0.0\n",
    "    \n",
    "    return 2.0 * intersection / denominator\n",
    "\n",
    "\n",
    "def compute_hausdorff95(pred_mask, target_mask, spacing=(1, 1, 1)):\n",
    "    \"\"\"\n",
    "    Compute 95th percentile Hausdorff distance.\n",
    "    \n",
    "    Args:\n",
    "        pred: Predicted mask\n",
    "        target: Ground truth mask\n",
    "        spacing: Voxel spacing\n",
    "    \n",
    "    Returns:\n",
    "        HD95 distance\n",
    "    \"\"\"\n",
    "    # pred_mask = (pred == label_idx).astype(np.uint8)\n",
    "    # target_mask = (target == label_idx).astype(np.uint8)\n",
    "    \n",
    "    if pred_mask.sum() == 0 or target_mask.sum() == 0:\n",
    "        return 0.0 if np.array_equal(pred_mask, target_mask) else 373.13  # Max distance\n",
    "    \n",
    "    try:\n",
    "        from scipy.spatial.distance import directed_hausdorff\n",
    "        \n",
    "        # Get boundary voxels\n",
    "        pred_surface = ndimage.binary_erosion(pred_mask) != pred_mask\n",
    "        target_surface = ndimage.binary_erosion(target_mask) != target_mask\n",
    "        \n",
    "        pred_coords = np.array(np.where(pred_surface)).T * spacing\n",
    "        target_coords = np.array(np.where(target_surface)).T * spacing\n",
    "        \n",
    "        if len(pred_coords) == 0 or len(target_coords) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        d1 = directed_hausdorff(pred_coords, target_coords)[0]\n",
    "        d2 = directed_hausdorff(target_coords, pred_coords)[0]\n",
    "        return max(d1, d2)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def evaluate_brats_metrics(pred, target):\n",
    "    \"\"\"\n",
    "    Compute BraTS evaluation metrics for all regions.\n",
    "    \n",
    "    Args:\n",
    "        pred: Predicted segmentation (H, W, D)\n",
    "        target: Ground truth segmentation (H, W, D)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with Dice scores for WT, TC, ET\n",
    "    \"\"\"\n",
    "    regions = {\n",
    "        'WT': (BraTSLabels.get_region_mask(target, 'WT'),\n",
    "                BraTSLabels.get_region_mask(pred, 'WT')),\n",
    "        'TC': (BraTSLabels.get_region_mask(target, 'TC'),\n",
    "                BraTSLabels.get_region_mask(pred, 'TC')),\n",
    "        'ET': (BraTSLabels.get_region_mask(target, 'ET'),\n",
    "                BraTSLabels.get_region_mask(pred, 'ET')),\n",
    "    }\n",
    "    \n",
    "    metrics = {}\n",
    "    for region, (target_mask, pred_mask) in regions.items():\n",
    "        # --- Dice ---\n",
    "        metrics[f'Dice_{region}'] = compute_dice_score(pred_mask, target_mask)\n",
    "        # --- 95th percentile Hausdorff distance ---\n",
    "        metrics[f\"HD95_{region}\"] = compute_hausdorff95(pred_mask, target_mask)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"✓ Metric functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Training Pipeline\n",
    "\n",
    "⚠️ **Warning: Full training may take 30-60+ minutes on a 24GB GPU.**  \n",
    "To test the pipeline quickly, enable `DEBUG_MODE` in the Config cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG MODE: Using only 2 subjects\n",
      "\n",
      "✓ Dataset Split:\n",
      "  Total subjects: 2\n",
      "  Training: 1\n",
      "  Validation: 1\n",
      "\n",
      "  Training subjects: ['BraTS20_Training_001']...\n",
      "  Validation subjects: ['BraTS20_Training_002']\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data: List subjects and split into train/val\n",
    "\n",
    "data_root = Path(Config.DATA_ROOT)\n",
    "\n",
    "# Extract subject IDs from image files\n",
    "all_subject_folders = sorted([\n",
    "    f.name for f in data_root.iterdir() \n",
    "    if f.is_dir() and f.name.startswith('BraTS20_Training')\n",
    "])\n",
    "\n",
    "if Config.DEBUG_MODE:\n",
    "    all_subject_folders = all_subject_folders[:2]  # Only 2 subjects for debugging\n",
    "    print(f\"DEBUG MODE: Using only {len(all_subject_folders)} subjects\")\n",
    "\n",
    "# Train/val split\n",
    "n_val = max(1, int(len(all_subject_folders) * Config.VALIDATION_SPLIT))\n",
    "train_subjects = all_subject_folders[:-n_val]\n",
    "val_subjects = all_subject_folders[-n_val:]\n",
    "\n",
    "print(f\"\\n✓ Dataset Split:\")\n",
    "print(f\"  Total subjects: {len(all_subject_folders)}\")\n",
    "print(f\"  Training: {len(train_subjects)}\")\n",
    "print(f\"  Validation: {len(val_subjects)}\")\n",
    "print(f\"\\n  Training subjects: {train_subjects[:3]}...\")\n",
    "print(f\"  Validation subjects: {val_subjects}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset (patch size: (128, 128, 128))...\n",
      "Creating validation dataset...\n",
      "\n",
      "✓ DataLoaders created:\n",
      "  Train batches per epoch: 2\n",
      "  Val batches: 1\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "\n",
    "batch_size = 1 if Config.DEBUG_MODE else Config.BATCH_SIZE\n",
    "num_epochs = 1 if Config.DEBUG_MODE else Config.NUM_EPOCHS\n",
    "\n",
    "print(f\"Creating training dataset (patch size: {Config.PATCH_SIZE})...\")\n",
    "train_dataset = BraTSDataset(\n",
    "    data_root=data_root,\n",
    "    subject_folder_names=train_subjects,\n",
    "    patch_size=Config.PATCH_SIZE,\n",
    "    augment=False,\n",
    "    num_patches_per_subject=2  # 2 random patches per subject per epoch\n",
    ")\n",
    "\n",
    "print(f\"Creating validation dataset...\")\n",
    "val_dataset = BraTSDataset(\n",
    "    data_root=data_root,\n",
    "    subject_folder_names=val_subjects,\n",
    "    patch_size=Config.PATCH_SIZE,\n",
    "    augment=False,\n",
    "    num_patches_per_subject=1\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ DataLoaders created:\")\n",
    "print(f\"  Train batches per epoch: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training and validation functions loaded\n"
     ]
    }
   ],
   "source": [
    "# Training Loop Function\n",
    "\n",
    "def train_epoch(model, train_loader, loss_fn, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        Average loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with tqdm(train_loader, desc=\"Training\") as pbar:\n",
    "        for batch in pbar:\n",
    "            images = batch['image'].to(device) # [B, C, H, W, D]\n",
    "            labels = batch['label'].to(device) # [B, H, W, D]\n",
    "            \n",
    "            # Add channel dimension for MONAI DiceCELoss\n",
    "            labels = labels.unsqueeze(1)  # → [B, 1, H, W, D]\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Validate for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        Average loss and metrics dictionary\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_metrics = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(val_loader, desc=\"Validation\") as pbar:\n",
    "            for batch in pbar:\n",
    "                images = batch['image'].to(device) # [B, C, H, W, D]\n",
    "                labels = batch['label'].to(device) # [B, H, W, D]\n",
    "                \n",
    "                # Add channel dim for loss computation\n",
    "                labels_loss = labels.unsqueeze(1)     # [B, 1, H, W, D]\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, labels_loss)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Compute metrics for this batch\n",
    "                # For metric evaluation, you still want original label (no channel dim)\n",
    "                # pred: [B, H, W, D] after argmax\n",
    "                pred = torch.argmax(outputs, dim=1).cpu().numpy() # [B, H, W, D]\n",
    "                target = labels.cpu().numpy()\n",
    "                \n",
    "                # Compute metrics per sample in batch (loop over B):\n",
    "                for b in range(pred.shape[0]):\n",
    "                    metrics = evaluate_brats_metrics(pred[b], target[b])\n",
    "                    for key, value in metrics.items():\n",
    "                        if key not in all_metrics:\n",
    "                            all_metrics[key] = []\n",
    "                        all_metrics[key].append(float(value))\n",
    "                \n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    avg_metrics = {k: np.mean(v) if v else 0.0 for k, v in all_metrics.items()}\n",
    "    \n",
    "    return avg_loss, avg_metrics\n",
    "\n",
    "print(\"✓ Training and validation functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING TRAINING | 1 epochs | cuda\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_start: 102\n",
      "h_start: 51\n",
      "h_start: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 1/2 [00:01<00:01,  1.04s/it, loss=2.3648]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_start: 102\n",
      "h_start: 51\n",
      "h_start: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s, loss=2.2696]\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_start: 102\n",
      "h_start: 51\n",
      "h_start: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it, loss=2.3043]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Train Loss: 2.3172\n",
      "  Val Loss: 2.3043\n",
      "  Dice_WT: 0.0076\n",
      "  Dice_TC: 0.0000\n",
      "  Dice_ET: 0.0000\n",
      "  LR: 9.96e-04\n",
      "  ✓ Best model saved: checkpoint\\best_model.pth\n",
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETE\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Main Training Loop\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"STARTING TRAINING | {num_epochs} epochs | {device}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_metrics': {}\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = Path(Config.CHECKPOINT_DIR) / 'best_model.pth'\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "        \n",
    "        # Training\n",
    "        train_loss = train_epoch(model, train_loader, loss_function, optimizer, device)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_metrics = validate_epoch(model, val_loader, loss_function, device)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        for key, value in val_metrics.items():\n",
    "            if key not in history['val_metrics']:\n",
    "                history['val_metrics'][key] = []\n",
    "            history['val_metrics'][key].append(value)\n",
    "        \n",
    "        # Learning rate step\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Dice_WT: {val_metrics['Dice_WT']:.4f}\")\n",
    "        print(f\"  Dice_TC: {val_metrics['Dice_TC']:.4f}\")\n",
    "        print(f\"  Dice_ET: {val_metrics['Dice_ET']:.4f}\")\n",
    "        print(f\"  LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"  ✓ Best model saved: {best_model_path}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n⚠ Training interrupted by user\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training history saved to ./output/training_history.png\n"
     ]
    }
   ],
   "source": [
    "# Plot training history\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Dice metrics\n",
    "axes[1].plot(history['val_metrics']['Dice_WT'], label='Dice_WT', marker='o')\n",
    "axes[1].plot(history['val_metrics']['Dice_TC'], label='Dice_TC', marker='s')\n",
    "axes[1].plot(history['val_metrics']['Dice_ET'], label='Dice_ET', marker='^')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Dice Score')\n",
    "axes[1].set_title('Validation Dice Scores by Region')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(Config.OUTPUT_DIR) / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Training history saved to {Config.OUTPUT_DIR}/training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Inference & Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Best model loaded from checkpoint\\best_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Load best model for inference\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "model.eval()\n",
    "print(f\"✓ Best model loaded from {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sliding window inference function loaded\n"
     ]
    }
   ],
   "source": [
    "# Inference Functions: Sliding Window Prediction\n",
    "\n",
    "def sliding_window_inference(\n",
    "    model,\n",
    "    volumes,\n",
    "    patch_size=(128, 128, 128),\n",
    "    overlap=0.5,\n",
    "    device='cuda'\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform sliding window inference on a full 3D volume.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        volumes: (C, H, W, D) input volume\n",
    "        patch_size: Patch size for each inference step\n",
    "        overlap: Overlap ratio between patches (0-1)\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        Predicted segmentation (H, W, D) with class indices\n",
    "    \"\"\"\n",
    "    c, h, w, d = volumes.shape\n",
    "    ph, pw, pd = patch_size\n",
    "    \n",
    "    # Initialize output and weight maps\n",
    "    output_shape = (4, h, w, d)  # 4 classes\n",
    "    output = np.zeros(output_shape, dtype=np.float32)\n",
    "    weights = np.zeros((h, w, d), dtype=np.float32)\n",
    "    \n",
    "    # Compute stride from overlap\n",
    "    stride_h = int(ph * (1 - overlap))\n",
    "    stride_w = int(pw * (1 - overlap))\n",
    "    stride_d = int(pd * (1 - overlap))\n",
    "    \n",
    "    stride_h = max(1, stride_h)\n",
    "    stride_w = max(1, stride_w)\n",
    "    stride_d = max(1, stride_d)\n",
    "    \n",
    "    # Sliding window loop\n",
    "    positions = []\n",
    "    for i in range(0, h - ph + 1, stride_h):\n",
    "        for j in range(0, w - pw + 1, stride_w):\n",
    "            for k in range(0, d - pd + 1, stride_d):\n",
    "                positions.append((i, j, k))\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if h > ph:\n",
    "        positions.append((h - ph, 0, 0))\n",
    "    if w > pw:\n",
    "        positions.append((0, w - pw, 0))\n",
    "    if d > pd:\n",
    "        positions.append((0, 0, d - pd))\n",
    "    \n",
    "    positions = list(set(positions))  # Remove duplicates\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (i, j, k) in enumerate(tqdm(positions, desc=\"Sliding Window\")):\n",
    "            # Extract patch\n",
    "            patch = volumes[:, i:i+ph, j:j+pw, k:k+pd]\n",
    "            \n",
    "            # Pad if necessary\n",
    "            if patch.shape != (c, ph, pw, pd):\n",
    "                pad_volumes = np.zeros((c, ph, pw, pd), dtype=np.float32)\n",
    "                pad_volumes[\n",
    "                    :,\n",
    "                    :patch.shape[1],\n",
    "                    :patch.shape[2],\n",
    "                    :patch.shape[3]\n",
    "                ] = patch\n",
    "                patch = pad_volumes\n",
    "            \n",
    "            # Forward pass\n",
    "            patch_tensor = torch.from_numpy(patch[None]).to(device).float()  # (1, C, H, W, D)\n",
    "            patch_output = model(patch_tensor).cpu().numpy()[0]  # (4, H, W, D)\n",
    "            \n",
    "            # Accumulate\n",
    "            output[:, i:i+ph, j:j+pw, k:k+pd] += patch_output\n",
    "            weights[i:i+ph, j:j+pw, k:k+pd] += 1.0\n",
    "    \n",
    "    # Average\n",
    "    weights = np.maximum(weights, 1.0)\n",
    "    output = output / weights[None]\n",
    "    \n",
    "    # Argmax to get class indices\n",
    "    prediction = np.argmax(output, axis=0).astype(np.uint8)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "print(\"✓ Sliding window inference function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Post-processing functions loaded\n"
     ]
    }
   ],
   "source": [
    "# Post-processing: Remove Small Connected Components\n",
    "\n",
    "def remove_small_components(segmentation, min_size=100):\n",
    "    \"\"\"\n",
    "    Remove small connected components from segmentation.\n",
    "    Helps reduce false positive detections.\n",
    "    \n",
    "    Args:\n",
    "        segmentation: (H, W, D) segmentation array\n",
    "        min_size: Minimum component size (in voxels)\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned segmentation\n",
    "    \"\"\"\n",
    "    segmentation = segmentation.copy()\n",
    "    labels = np.unique(segmentation)\n",
    "    print(f\"Labels: {labels}\")\n",
    "    \n",
    "    for label in np.unique(segmentation):\n",
    "        if label == 0:  # Skip background\n",
    "            continue\n",
    "        \n",
    "        mask = (segmentation == label)\n",
    "        labeled, num_features = ndimage.label(mask)\n",
    "        \n",
    "        print(f\"Label {label}: {num_features} components\")\n",
    "        \n",
    "        for component_id in tqdm(range(1, num_features + 1), desc=f\"Label {label}\", leave=False, unit=\"comp\"):\n",
    "            component_size = np.sum(labeled == component_id)\n",
    "            if component_size < min_size:\n",
    "                segmentation[labeled == component_id] = 0\n",
    "    \n",
    "    return segmentation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_volume(model, subject_path, affine, device=torch.device('cuda')):\n",
    "    \"\"\"\n",
    "    Full inference pipeline for a single subject.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        subject_path: Path to subject data\n",
    "        affine: Affine matrix for NIfTI\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        Predicted segmentation (H, W, D)\n",
    "    \"\"\"\n",
    "    # Sliding window inference\n",
    "    print(\"Running sliding window inference...\")\n",
    "    prediction = sliding_window_inference(\n",
    "        model, subject_path, patch_size=Config.INFERENCE_PATCH_SIZE,\n",
    "        overlap=Config.INFERENCE_OVERLAP, device=device\n",
    "    )\n",
    "    \n",
    "    # Post-processing: Remove small components\n",
    "    print(\"Applying post-processing...\")\n",
    "    prediction = remove_small_components(prediction, min_size=100)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "print(\"✓ Post-processing functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test subject: BraTS20_Training_002\n",
      "Volume shape: (4, 240, 240, 155)\n",
      "Label shape: (240, 240, 155)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window: 100%|██████████| 7/7 [00:01<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [0 1 2 3]\n",
      "Label 1: 337077 components\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 27\u001b[0m\n\u001b[0;32m     19\u001b[0m     test_prediction \u001b[38;5;241m=\u001b[39m sliding_window_inference(\n\u001b[0;32m     20\u001b[0m         model, test_volumes, \n\u001b[0;32m     21\u001b[0m         patch_size\u001b[38;5;241m=\u001b[39mConfig\u001b[38;5;241m.\u001b[39mINFERENCE_PATCH_SIZE,\n\u001b[0;32m     22\u001b[0m         overlap\u001b[38;5;241m=\u001b[39mConfig\u001b[38;5;241m.\u001b[39mINFERENCE_OVERLAP, \n\u001b[0;32m     23\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m     24\u001b[0m     )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Post-process\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m test_prediction_cleaned \u001b[38;5;241m=\u001b[39m remove_small_components(test_prediction, min_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ Inference complete. Prediction shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_prediction_cleaned\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Compute metrics\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 31\u001b[0m, in \u001b[0;36mremove_small_components\u001b[1;34m(segmentation, min_size)\u001b[0m\n\u001b[0;32m     29\u001b[0m         component_size \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(labeled \u001b[38;5;241m==\u001b[39m component_id)\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m component_size \u001b[38;5;241m<\u001b[39m min_size:\n\u001b[1;32m---> 31\u001b[0m             segmentation[labeled \u001b[38;5;241m==\u001b[39m component_id] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m segmentation\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example Inference on Validation Subject\n",
    "\n",
    "# Select a validation subject\n",
    "if len(val_subjects) > 0:\n",
    "    test_subject_id = val_subjects[0]\n",
    "    print(f\"Loading test subject: {test_subject_id}\")\n",
    "    \n",
    "    # Load subject data\n",
    "    test_data = load_subject_volumes(Config.DATA_ROOT, test_subject_id)\n",
    "    test_volumes = preprocess_subject(test_data)['volumes']  # (4, H, W, D)\n",
    "    test_label = test_data['label']  # (H, W, D)\n",
    "    test_affine = test_data['affine']\n",
    "    \n",
    "    print(f\"Volume shape: {test_volumes.shape}\")\n",
    "    print(f\"Label shape: {test_label.shape}\")\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        test_prediction = sliding_window_inference(\n",
    "            model, test_volumes, \n",
    "            patch_size=Config.INFERENCE_PATCH_SIZE,\n",
    "            overlap=Config.INFERENCE_OVERLAP, \n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    # Post-process\n",
    "    test_prediction_cleaned = remove_small_components(test_prediction, min_size=50)\n",
    "    \n",
    "    print(f\"\\n✓ Inference complete. Prediction shape: {test_prediction_cleaned.shape}\")\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = evaluate_brats_metrics(test_prediction_cleaned, test_label)\n",
    "    print(f\"\\nMetrics on test subject {test_subject_id}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    for region in ['WT', 'TC', 'ET']:\n",
    "        print(f\"  {region}: Dice={metrics[f'Dice_{region}']:.4f}, HD95={metrics[f'HD95_{region}']:.2f}\")\n",
    "else:\n",
    "    print(\"No validation subjects available for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Visualization of Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Helper Function\n",
    "\n",
    "def visualize_segmentation(\n",
    "    volumes,\n",
    "    prediction,\n",
    "    ground_truth,\n",
    "    slice_idx=None,\n",
    "    axis=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize a slice with prediction and ground truth overlaid.\n",
    "    \n",
    "    Args:\n",
    "        volumes: (C, H, W, D) MRI volumes\n",
    "        prediction: (H, W, D) predicted segmentation\n",
    "        ground_truth: (H, W, D) ground truth segmentation\n",
    "        slice_idx: Slice index to visualize (default: middle slice)\n",
    "        axis: Axis to slice along (0=axial, 1=coronal, 2=sagittal)\n",
    "    \"\"\"\n",
    "    # Select middle slice if not specified\n",
    "    if slice_idx is None:\n",
    "        slice_idx = volumes.shape[axis + 1] // 2\n",
    "    \n",
    "    # Extract slice\n",
    "    if axis == 0:  # Axial\n",
    "        img_slice = volumes[0, slice_idx]  # Use T1\n",
    "        pred_slice = prediction[slice_idx]\n",
    "        gt_slice = ground_truth[slice_idx]\n",
    "        title_suffix = f\"Axial (slice {slice_idx})\"\n",
    "    elif axis == 1:  # Coronal\n",
    "        img_slice = volumes[0, :, slice_idx]\n",
    "        pred_slice = prediction[:, slice_idx]\n",
    "        gt_slice = ground_truth[:, slice_idx]\n",
    "        title_suffix = f\"Coronal (slice {slice_idx})\"\n",
    "    else:  # Sagittal\n",
    "        img_slice = volumes[0, :, :, slice_idx]\n",
    "        pred_slice = prediction[:, :, slice_idx]\n",
    "        gt_slice = ground_truth[:, :, slice_idx]\n",
    "        title_suffix = f\"Sagittal (slice {slice_idx})\"\n",
    "    \n",
    "    # Color map for labels\n",
    "    cmap = {\n",
    "        0: [0, 0, 0],  # Background (black)\n",
    "        1: [1, 0, 0],  # NCR/NET (red)\n",
    "        2: [0, 1, 0],  # ED (green)\n",
    "        3: [0, 0, 1],  # ET (blue)\n",
    "    }\n",
    "    \n",
    "    def convert_to_rgb(seg):\n",
    "        rgb = np.zeros((*seg.shape, 3), dtype=np.float32)\n",
    "        for label, color in cmap.items():\n",
    "            mask = seg == label\n",
    "            for c in range(3):\n",
    "                rgb[..., c][mask] = color[c]\n",
    "        return rgb\n",
    "    \n",
    "    pred_rgb = convert_to_rgb(pred_slice)\n",
    "    gt_rgb = convert_to_rgb(gt_slice)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # MRI image\n",
    "    axes[0].imshow(img_slice, cmap='gray')\n",
    "    axes[0].set_title(f\"T1 - {title_suffix}\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[1].imshow(img_slice, cmap='gray')\n",
    "    axes[1].imshow(gt_rgb, alpha=0.6)\n",
    "    axes[1].set_title(f\"Ground Truth - {title_suffix}\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    axes[2].imshow(img_slice, cmap='gray')\n",
    "    axes[2].imshow(pred_rgb, alpha=0.6)\n",
    "    axes[2].set_title(f\"Prediction - {title_suffix}\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Legend\n",
    "    handles = [\n",
    "        mpatches.Patch(color=[1, 0, 0], label='NCR/NET'),\n",
    "        mpatches.Patch(color=[0, 1, 0], label='ED'),\n",
    "        mpatches.Patch(color=[0, 0, 1], label='ET'),\n",
    "    ]\n",
    "    fig.legend(handles=handles, loc='upper center', ncol=3, bbox_to_anchor=(0.5, -0.02))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"✓ Visualization function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "\n",
    "if len(val_subjects) > 0:\n",
    "    # Axial view\n",
    "    fig_axial = visualize_segmentation(\n",
    "        test_volumes, test_prediction_cleaned, test_label, axis=0\n",
    "    )\n",
    "    plt.savefig(Path(Config.OUTPUT_DIR) / f'{test_subject_id}_axial.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Coronal view\n",
    "    fig_cor = visualize_segmentation(\n",
    "        test_volumes, test_prediction_cleaned, test_label, axis=1\n",
    "    )\n",
    "    plt.savefig(Path(Config.OUTPUT_DIR) / f'{test_subject_id}_coronal.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Sagittal view\n",
    "    fig_sag = visualize_segmentation(\n",
    "        test_volumes, test_prediction_cleaned, test_label, axis=2\n",
    "    )\n",
    "    plt.savefig(Path(Config.OUTPUT_DIR) / f'{test_subject_id}_sagittal.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Visualizations saved to {Config.OUTPUT_DIR}/\")\n",
    "else:\n",
    "    print(\"No validation subjects available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Uncertainty Estimation (Monte Carlo Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo Dropout for Uncertainty Estimation\n",
    "\n",
    "def monte_carlo_dropout_inference(\n",
    "    model,\n",
    "    volumes,\n",
    "    n_passes=20,\n",
    "    patch_size=(128, 128, 128),\n",
    "    overlap=0.5,\n",
    "    device='cuda'\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform MC Dropout inference: run multiple forward passes with dropout enabled.\n",
    "    Compute mean and variance of predictions to estimate uncertainty.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model (with dropout layers)\n",
    "        volumes: (C, H, W, D) input volume\n",
    "        n_passes: Number of forward passes\n",
    "        patch_size: Patch size\n",
    "        overlap: Overlap ratio\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        pred_mean: Mean prediction (H, W, D, C)\n",
    "        pred_std: Std of predictions (H, W, D)\n",
    "    \"\"\"\n",
    "    c, h, w, d = volumes.shape\n",
    "    \n",
    "    # Store all predictions\n",
    "    predictions = []\n",
    "    \n",
    "    print(f\"Running {n_passes} MC Dropout passes...\")\n",
    "    for pass_idx in tqdm(range(n_passes), desc=\"MC Dropout\"):\n",
    "        model.train()  # Enable dropout\n",
    "        with torch.no_grad():\n",
    "            pred = sliding_window_inference(\n",
    "                model, volumes, patch_size=patch_size,\n",
    "                overlap=overlap, device=device\n",
    "            )\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # Stack predictions: (n_passes, H, W, D)\n",
    "    predictions = np.stack(predictions, axis=0)\n",
    "    \n",
    "    # Compute statistics\n",
    "    pred_mean = predictions.mean(axis=0)\n",
    "    pred_std = predictions.std(axis=0)\n",
    "    \n",
    "    # Normalize uncertainty to [0, 1]\n",
    "    pred_std = (pred_std - pred_std.min()) / (pred_std.max() - pred_std.min() + 1e-8)\n",
    "    \n",
    "    model.eval()  # Disable dropout\n",
    "    \n",
    "    return pred_mean, pred_std\n",
    "\n",
    "print(\"✓ MC Dropout uncertainty estimation function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MC Dropout on test subject (skip if computationally expensive)\n",
    "\n",
    "# To save computation time, we can skip this in DEBUG_MODE or for initial testing\n",
    "# Uncomment below to run uncertainty estimation\n",
    "\n",
    "RUN_MC_DROPOUT = not Config.DEBUG_MODE  # Skip in debug mode\n",
    "\n",
    "if RUN_MC_DROPOUT and len(val_subjects) > 0:\n",
    "    print(\"⏳ Computing uncertainty estimates (this may take a while)...\\n\")\n",
    "    \n",
    "    pred_mean, pred_std = monte_carlo_dropout_inference(\n",
    "        model, test_volumes,\n",
    "        n_passes=Config.MC_DROPOUT_PASSES,\n",
    "        patch_size=Config.INFERENCE_PATCH_SIZE,\n",
    "        overlap=Config.INFERENCE_OVERLAP,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Uncertainty estimation complete\")\n",
    "    print(f\"  Mean prediction shape: {pred_mean.shape}\")\n",
    "    print(f\"  Std map shape: {pred_std.shape}\")\n",
    "elif not RUN_MC_DROPOUT:\n",
    "    print(\"⏭️ Skipping MC Dropout (DEBUG_MODE enabled)\")\n",
    "else:\n",
    "    print(\"No validation subjects available for uncertainty estimation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize uncertainty map\n",
    "\n",
    "if RUN_MC_DROPOUT and len(val_subjects) > 0:\n",
    "    # Get mean prediction from MC samples\n",
    "    mc_prediction = np.argmax(pred_mean, axis=-1).astype(np.uint8)\n",
    "    \n",
    "    # Visualize uncertainty\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    slice_idx = test_volumes.shape[1] // 2\n",
    "    \n",
    "    # Axial\n",
    "    img_slice = test_volumes[0, slice_idx]\n",
    "    unc_slice = pred_std[slice_idx]\n",
    "    \n",
    "    axes[0, 0].imshow(img_slice, cmap='gray')\n",
    "    axes[0, 0].set_title(\"MRI (Axial)\")\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    im = axes[0, 1].imshow(unc_slice, cmap='hot')\n",
    "    axes[0, 1].set_title(\"Uncertainty Map (Axial)\")\n",
    "    axes[0, 1].axis('off')\n",
    "    plt.colorbar(im, ax=axes[0, 1], label='Std Dev')\n",
    "    \n",
    "    axes[0, 2].imshow(img_slice, cmap='gray')\n",
    "    axes[0, 2].imshow(unc_slice, cmap='hot', alpha=0.5)\n",
    "    axes[0, 2].set_title(\"Uncertainty Overlay (Axial)\")\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # Coronal\n",
    "    slice_idx_cor = test_volumes.shape[2] // 2\n",
    "    img_slice_cor = test_volumes[0, :, slice_idx_cor]\n",
    "    unc_slice_cor = pred_std[:, slice_idx_cor]\n",
    "    \n",
    "    axes[1, 0].imshow(img_slice_cor, cmap='gray')\n",
    "    axes[1, 0].set_title(\"MRI (Coronal)\")\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    im = axes[1, 1].imshow(unc_slice_cor, cmap='hot')\n",
    "    axes[1, 1].set_title(\"Uncertainty Map (Coronal)\")\n",
    "    axes[1, 1].axis('off')\n",
    "    plt.colorbar(im, ax=axes[1, 1], label='Std Dev')\n",
    "    \n",
    "    axes[1, 2].imshow(img_slice_cor, cmap='gray')\n",
    "    axes[1, 2].imshow(unc_slice_cor, cmap='hot', alpha=0.5)\n",
    "    axes[1, 2].set_title(\"Uncertainty Overlay (Coronal)\")\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(Config.OUTPUT_DIR) / f'{test_subject_id}_uncertainty.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Uncertainty visualization saved to {Config.OUTPUT_DIR}/\")\n",
    "else:\n",
    "    print(\"MC Dropout not run or no validation subjects available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 10: Save Predictions as NIfTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions as NIfTI files\n",
    "\n",
    "def save_segmentation_nifti(segmentation, affine, output_path, description=\"\"):\n",
    "    \"\"\"\n",
    "    Save segmentation as NIfTI file.\n",
    "    \n",
    "    Args:\n",
    "        segmentation: (H, W, D) segmentation array\n",
    "        affine: (4, 4) affine transformation matrix\n",
    "        output_path: Path to save NIfTI file\n",
    "        description: Optional description\n",
    "    \"\"\"\n",
    "    img = nib.Nifti1Image(segmentation, affine=affine)\n",
    "    img.header.set_data_dtype(np.uint8)\n",
    "    if description:\n",
    "        img.header['descrip'] = description.encode('utf-8')[:80]\n",
    "    nib.save(img, output_path)\n",
    "    print(f\"✓ Saved: {output_path}\")\n",
    "\n",
    "# Save test subject prediction\n",
    "if len(val_subjects) > 0:\n",
    "    output_path = Path(Config.OUTPUT_DIR) / f'{test_subject_id}_pred.nii.gz'\n",
    "    save_segmentation_nifti(\n",
    "        test_prediction_cleaned,\n",
    "        test_affine,\n",
    "        output_path,\n",
    "        description=\"3D U-Net Segmentation Prediction\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n✓ Predictions saved to {Config.OUTPUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 11: Cross-Validation Setup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-Fold Cross-Validation Framework\n",
    "\n",
    "def run_cross_validation(\n",
    "    subject_ids,\n",
    "    n_folds=5,\n",
    "    epochs_per_fold=10,\n",
    "    fold_idx=0  # Which fold to run (default: 0)\n",
    "):\n",
    "    \"\"\"\n",
    "    Run k-fold cross-validation.\n",
    "    This is a framework function; actual execution is computationally intensive.\n",
    "    \n",
    "    Args:\n",
    "        subject_ids: List of all subject IDs\n",
    "        n_folds: Number of folds\n",
    "        epochs_per_fold: Epochs per fold\n",
    "        fold_idx: Which fold to run\n",
    "    \n",
    "    Returns:\n",
    "        Cross-validation results\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    cv_results = {}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(subject_ids)):\n",
    "        if fold != fold_idx:\n",
    "            continue  # Skip other folds\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FOLD {fold + 1}/{n_folds}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Get train/val splits\n",
    "        train_subjects_fold = [subject_ids[i] for i in train_idx]\n",
    "        val_subjects_fold = [subject_ids[i] for i in val_idx]\n",
    "        \n",
    "        print(f\"Train subjects: {len(train_subjects_fold)}\")\n",
    "        print(f\"Val subjects: {len(val_subjects_fold)}\")\n",
    "        \n",
    "        # [Training code would go here - similar to main training loop]\n",
    "        # For now, this is just a framework demonstration\n",
    "        \n",
    "        cv_results[f'fold_{fold}'] = {\n",
    "            'train_subjects': train_subjects_fold,\n",
    "            'val_subjects': val_subjects_fold,\n",
    "            'status': 'Not Run'\n",
    "        }\n",
    "    \n",
    "    return cv_results\n",
    "\n",
    "print(\"✓ Cross-validation framework loaded\")\n",
    "print(\"\\nTo run full 5-fold cross-validation:\")\n",
    "print(\"  cv_results = run_cross_validation(all_subjects, n_folds=5, fold_idx=0)\")\n",
    "print(\"  (Repeat for fold_idx=1,2,3,4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 12: Export & Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to CSV\n",
    "\n",
    "# Prepare metrics dataframe\n",
    "metrics_data = []\n",
    "for epoch in range(len(history['train_loss'])):\n",
    "    metrics_data.append({\n",
    "        'Epoch': epoch + 1,\n",
    "        'Train_Loss': history['train_loss'][epoch],\n",
    "        'Val_Loss': history['val_loss'][epoch],\n",
    "        'Dice_WT': history['val_metrics']['Dice_WT'][epoch],\n",
    "        'Dice_TC': history['val_metrics']['Dice_TC'][epoch],\n",
    "        'Dice_ET': history['val_metrics']['Dice_ET'][epoch],\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_path = Path(Config.OUTPUT_DIR) / 'training_metrics.csv'\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "print(f\"✓ Metrics saved to {metrics_path}\")\n",
    "print(\"\\nMetrics Summary:\")\n",
    "print(metrics_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "# BraTS-2020 3D U-Net Segmentation - Training Summary\n",
    "\n",
    "## Configuration\n",
    "- Patch Size: {Config.PATCH_SIZE}\n",
    "- Batch Size: {Config.BATCH_SIZE}\n",
    "- Epochs: {len(history['train_loss'])}\n",
    "- Learning Rate: {Config.LEARNING_RATE}\n",
    "- Device: {device}\n",
    "\n",
    "## Dataset\n",
    "- Total Subjects: {len(all_subject_folders)}\n",
    "- Training: {len(train_subjects)}\n",
    "- Validation: {len(val_subjects)}\n",
    "\n",
    "## Model\n",
    "- Architecture: 3D U-Net (MONAI)\n",
    "- Input Channels: {Config.IN_CHANNELS} (T1, T1ce, T2, FLAIR)\n",
    "- Output Classes: {Config.OUT_CHANNELS} (Background, NCR/NET, ED, ET)\n",
    "- Total Parameters: {total_params:,}\n",
    "- Trainable Parameters: {trainable_params:,}\n",
    "\n",
    "## Training Results\n",
    "- Best Val Loss: {min(history['val_loss']):.4f}\n",
    "- Final Dice_WT: {history['val_metrics']['Dice_WT'][-1]:.4f}\n",
    "- Final Dice_TC: {history['val_metrics']['Dice_TC'][-1]:.4f}\n",
    "- Final Dice_ET: {history['val_metrics']['Dice_ET'][-1]:.4f}\n",
    "\n",
    "## Files Generated\n",
    "- Model Checkpoint: {best_model_path}\n",
    "- Metrics CSV: {metrics_path}\n",
    "- Visualizations: {Config.OUTPUT_DIR}/\n",
    "- Training History Plot: {Config.OUTPUT_DIR}/training_history.png\n",
    "\n",
    "## Notes\n",
    "- Preprocessing: Z-score normalization per modality, brain mask-aware\n",
    "- Loss Function: DiceCELoss (Dice + Cross Entropy)\n",
    "- Optimizer: AdamW with CosineAnnealingLR\n",
    "- Inference: Sliding-window with post-processing (small component removal)\n",
    "\"\"\"\n",
    "\n",
    "summary_path = Path(Config.OUTPUT_DIR) / 'SUMMARY.md'\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(f\"✓ Summary report saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output archive\n",
    "\n",
    "import zipfile\n",
    "\n",
    "archive_name = 'BraTS_3DUNet_Results.zip'\n",
    "archive_path = Path(archive_name)\n",
    "\n",
    "with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    # Add model checkpoint\n",
    "    if best_model_path.exists():\n",
    "        zf.write(best_model_path, arcname='model/best_model.pth')\n",
    "    \n",
    "    # Add all outputs\n",
    "    for file in Path(Config.OUTPUT_DIR).glob('*'):\n",
    "        if file.is_file():\n",
    "            zf.write(file, arcname=f'outputs/{file.name}')\n",
    "    \n",
    "    # Add summary\n",
    "    zf.write(summary_path, arcname='SUMMARY.md')\n",
    "\n",
    "print(f\"✓ Archive created: {archive_path}\")\n",
    "print(f\"  Size: {archive_path.stat().st_size / 1e6:.1f} MB\")\n",
    "print(f\"\\nTo download results, use: files.download('{archive_path}')  # Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 13: References & Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "**BraTS Dataset:**\n",
    "- Menze, B. H., et al. (2015). The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS). IEEE TMI, 34(8), 1993-2024.\n",
    "  - Dataset: https://www.med.upenn.edu/cbica/brats2020/\n",
    "  - Citation: arXiv:1811.02629 (BraTS 2020)\n",
    "\n",
    "**3D U-Net Architecture:**\n",
    "- Çiçek, Ö., Abdulkadir, A., Lienkamp, S. S., Brox, T., & Ronneberger, O. (2016). 3D U-Net: Learning dense volumetric segmentation from sparse annotation. MICCAI 2016.\n",
    "  - Implementation: MONAI (Medical Open Network for AI)\n",
    "\n",
    "**Loss Functions:**\n",
    "- Dice Loss: Milletari, F., Navab, N., & Ahmadi, S. A. (2016). The Dice coefficient for measuring segmentation accuracy. MICCAI 2016.\n",
    "- Combined Dice-CE Loss: Commonly used in medical imaging\n",
    "\n",
    "**Metrics:**\n",
    "- Hausdorff Distance: Hausdorff, F. (1914). Grundzüge der Mengenlehre.\n",
    "- HD95 is standard in BraTS evaluation\n",
    "\n",
    "**Libraries:**\n",
    "- MONAI: Cardoso, M. J., et al. (2022). MONAI: Medical Open Network for AI. arXiv:2211.02701\n",
    "- TorchIO: Pérez-García, F., et al. (2021). TorchIO: A Python Library for Efficient Loading, Preprocessing, Augmentation and Patch-Based Sampling of Medical Images. Computer Methods and Programs in Biomedicine. https://github.com/fepegar/torchio\n",
    "- PyTorch: Paszke, A., et al. (2019). PyTorch: An imperative style, high-performance deep learning library. NeurIPS 2019.\n",
    "\n",
    "**Uncertainty Estimation:**\n",
    "- MC Dropout: Gal, Y., & Ghahramani, Z. (2016). Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. ICML 2016.\n",
    "- QU-BraTS: https://www.med.upenn.edu/cbica/qu-brats/ (Uncertainty quantification challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Start Checklist\n",
    "\n",
    "### Before Running This Notebook:\n",
    "\n",
    "1. **Obtain BraTS-2020 Dataset**\n",
    "   - Register at: https://www.med.upenn.edu/cbica/brats2020/\n",
    "   - Download the training dataset (imagesTr + labelsTr)\n",
    "   - Extract to a local folder\n",
    "\n",
    "2. **Update Configuration**\n",
    "   - In Section 1 (Config cell), update `DATA_ROOT` to point to your BraTS folder\n",
    "   - Example: `DATA_ROOT = '/data/BraTS2020'`\n",
    "\n",
    "3. **GPU Setup** (Optional but recommended)\n",
    "   - Ensure CUDA 11.8+ is installed\n",
    "   - Have at least 8GB VRAM (16GB+ recommended)\n",
    "   - For smaller GPUs: reduce `PATCH_SIZE` to (96, 96, 96) or (64, 64, 64)\n",
    "\n",
    "4. **Test with Debug Mode** (Recommended)\n",
    "   - Set `DEBUG_MODE = True` in Config to run quickly on 2 subjects\n",
    "   - Verify everything works before running full training\n",
    "   - Runtime: ~5-10 minutes\n",
    "\n",
    "5. **Full Training**\n",
    "   - Set `DEBUG_MODE = False` to train on all subjects\n",
    "   - Set `NUM_EPOCHS` to desired value (e.g., 50)\n",
    "   - Estimated runtime: 30-60 minutes per epoch on 24GB GPU\n",
    "\n",
    "6. **Download Results**\n",
    "   - After training, Section 12 creates a zip archive\n",
    "   - In Colab: `files.download('BraTS_3DUNet_Results.zip')`\n",
    "   - In local notebook: file is saved in current directory\n",
    "\n",
    "### Runtime Notes:\n",
    "\n",
    "- **Colab Free Tier**: May timeout after 12 hours; save checkpoints frequently\n",
    "- **Batch Size**: Reduce if OOM errors occur\n",
    "- **Patch Size**: Reduce for smaller GPUs (128×128×128 → 96×96×96 or 64×64×64)\n",
    "- **Num Workers**: Set to 0 on Windows; increase on Linux for faster loading\n",
    "- **MC Dropout**: Skip during initial testing (computationally expensive)\n",
    "\n",
    "### Expected Outputs:\n",
    "\n",
    "```\n",
    "outputs/\n",
    "├── best_model.pth              # Trained model weights\n",
    "├── training_metrics.csv         # Loss & Dice scores per epoch\n",
    "├── training_history.png         # Loss and Dice curves\n",
    "├── {subject_id}_axial.png       # Prediction visualization (axial)\n",
    "├── {subject_id}_coronal.png     # Prediction visualization (coronal)\n",
    "├── {subject_id}_sagittal.png    # Prediction visualization (sagittal)\n",
    "├── {subject_id}_uncertainty.png # Uncertainty heatmaps (if MC Dropout run)\n",
    "└── {subject_id}_pred.nii.gz     # Predicted segmentation (NIfTI format)\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
